{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import argparse\n",
    "import imutils\n",
    "from scipy.spatial import distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up...\n",
    "confthreshold =0.5\n",
    "nmsthreshold = 0.3\n",
    "min_distance = 50\n",
    "modelconfiguration='yolov4.cfg'\n",
    "modelweight='yolov4.weights'\n",
    "classfile='coco.names'\n",
    "classnames=[]\n",
    "with open(classfile,'rt') as f:\n",
    "    classnames=f.read().rstrip('\\n').split('\\n')\n",
    "net = cv2.dnn.readNet(modelweight, modelconfiguration)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, net, ln, personIdx=0):\n",
    "\t# grab the dimensions of the frame and  initialize the list of\n",
    "\t# results\n",
    "\t(H, W) = frame.shape[:2]\n",
    "\tresults = []\n",
    "    # construct a blob from the input frame and then perform a forward\n",
    "\t# pass of the YOLO object detector, giving us our bounding boxes\n",
    "\t# and associated probabilities\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\tlayerOutputs = net.forward(ln)\n",
    "\t# initialize our lists of detected bounding boxes, centroids, and\n",
    "\t# confidences, respectively\n",
    "\tboxes = []\n",
    "\tcentroids = []\n",
    "\tconfidences = []\n",
    "    # loop over each of the layer outputs\n",
    "\tfor output in layerOutputs:\n",
    "\t\t# loop over each of the detections\n",
    "\t\tfor detection in output:\n",
    "\t\t\t# extract the class ID and confidence (i.e., probability)\n",
    "\t\t\t# of the current object detection\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\t\t\t# filter detections by (1) ensuring that the object\n",
    "\t\t\t# detected was a person and (2) that the minimum\n",
    "\t\t\t# confidence is met\n",
    "\t\t\tif classID == personIdx and confidence > confthreshold:\n",
    "\t\t\t\t# scale the bounding box coordinates back relative to\n",
    "\t\t\t\t# the size of the image, keeping in mind that YOLO\n",
    "\t\t\t\t# actually returns the center (x, y)-coordinates of\n",
    "\t\t\t\t# the bounding box followed by the boxes' width and\n",
    "\t\t\t\t# height\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\t\t\t\t# use the center (x, y)-coordinates to derive the top\n",
    "\t\t\t\t# and left corner of the bounding box\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\t\t\t\t# update our list of bounding box coordinates,\n",
    "\t\t\t\t# centroids, and confidences\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tcentroids.append((centerX, centerY))\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "    # apply non-maxima suppression to suppress weak, overlapping\n",
    "\t# bounding boxes\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, confthreshold, nmsthreshold)\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# loop over the indexes we are keeping\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\t\t\t# update our results list to consist of the person\n",
    "\t\t\t# prediction probability, bounding box coordinates,\n",
    "\t\t\t# and the centroid\n",
    "\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "\t\t\tresults.append(r)\n",
    "\t# return the list of results\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take in an video source and label rect and names for each object.\n",
    "def filedet(filename,wh):\n",
    "    cap=cv2.VideoCapture(filename)\n",
    "    prev_frame_time = 0\n",
    "    new_frame_time = 0\n",
    "    cv2.startWindowThread()\n",
    "    if (cap.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "    while cap.isOpened():\n",
    "        success,img=cap.read()\n",
    "        if success == True:\n",
    "            blob=cv2.dnn.blobFromImage(img,1/255,(wh,wh),[0,0,0],1,crop=False)\n",
    "            net.setInput(blob)\n",
    "            layerNames= net.getLayerNames()\n",
    "            outputNames = [layerNames[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "            outputs=net.forward(outputNames)\n",
    "            img = imutils.resize(img, width=700)\n",
    "#----------------------------------------------------------------------------------------\n",
    "            results = detect_people(img, net, outputNames,\n",
    "            personIdx=classnames.index(\"person\"))\n",
    "            violate = set()\n",
    "            if len(results) >= 2:\n",
    "                # extract all centroids from the results and compute the\n",
    "                # Euclidean distances between all pairs of the centroids\n",
    "                centroids = np.array([r[2] for r in results])\n",
    "                D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "                # loop over the upper triangular of the distance matrix\n",
    "                for i in range(0, D.shape[0]):\n",
    "                    for j in range(i + 1, D.shape[1]):\n",
    "                        # check to see if the distance between any two\n",
    "                        # centroid pairs is less than the configured number\n",
    "                        # of pixels\n",
    "                        if D[i, j] < min_distance:\n",
    "                            # update our violation set with the indexes of\n",
    "                            # the centroid pairs\n",
    "                            violate.add(i)\n",
    "                            violate.add(j)\n",
    "            for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "                # extract the bounding box and centroid coordinates, then\n",
    "                # initialize the color of the annotation\n",
    "                (startX, startY, endX, endY) = bbox\n",
    "                (cX, cY) = centroid\n",
    "                color = (0, 255, 0)\n",
    "                # if the index pair exists within the violation set, then\n",
    "                # update the color\n",
    "                if i in violate:\n",
    "                    color = (0, 0, 255)\n",
    "                # draw (1) a bounding box around the person and (2) the\n",
    "                # centroid coordinates of the person,\n",
    "                cv2.rectangle(img, (startX, startY), (endX, endY), color, 2)\n",
    "                cv2.circle(img, (cX, cY), 5, color, 1)\n",
    "            # draw the total number of social distancing violations on the\n",
    "            # output frame\n",
    "            text = \"Social Distancing Violations: {}\".format(len(violate))\n",
    "            cv2.putText(img, text, (10, img.shape[0] - 25),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)             \n",
    " #---------------------------------------------------------------------------------------------------               \n",
    "            #find fps\n",
    "            gray = img\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            new_frame_time = time.time()\n",
    "            fps = 1/(new_frame_time-prev_frame_time) \n",
    "            prev_frame_time = new_frame_time \n",
    "            fps = int(fps)\n",
    "            fps = str(fps)\n",
    "            cv2.putText(gray, fps, (7, 70), font, 1, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "            #open final window\n",
    "            cv2.imshow('Img',img)\n",
    "            #press q to quite the window\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedet(filename='pedestrians.mp4',wh=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
